# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.


# The configuration file needs to define the sources, 
# the channels and the sinks.
# Sources, channels and sinks are defined per agent, 
# in this case called 'end-collector'

####################################################################################################
#avro source => disk backed memory => kafka
####################################################################################################

end-collector.sources =<% @sources.each do |src, details| %><%= " #{src}" %><% end %>
end-collector.channels = <%= @allchannels %>
end-collector.sinks = <%= @allsinks %>


<% @sources.each do |src, details| %>
<% if details[:'src_category'] == 'dfw1kafkamerge' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw2kafkamerge' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'ams1kafkamerge' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['ams1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['ams1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'maa1kafkamerge' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['maa1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['maa1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'pek1kafkamerge' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['pek1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['pek1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'hdfslocal' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= @kafkazookeeper %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= @kafkabrokers %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = lbs_src
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>
<% if details[:'src_category'] == 'hdfsmerge' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= @kafkazookeeper %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= @kafkabrokers %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = mbs_src
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-remover-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-remover-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixRemover$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-remover-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>
<% if details[:'src_category'] == 'platinumhdfs' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= @kafkazookeeper %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= @kafkabrokers %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = global_merger_src
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal1' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal2' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal3' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal4' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal5' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal6' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal7' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal8' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw1eventhublocal9' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw1'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw2eventhublocal1' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>


<% if details[:'src_category'] == 'dfw2eventhublocal2' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw2eventhublocal3' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>


<% if details[:'src_category'] == 'dfw2eventhublocal4' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>


<% if details[:'src_category'] == 'dfw2eventhublocal5' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>


<% if details[:'src_category'] == 'dfw2eventhublocal6' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw2eventhublocal7' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>

<% if details[:'src_category'] == 'dfw2eventhublocal8' -%>
end-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
end-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
end-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["flume_collector"]["kafka_zookeeper"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.offsetStore = zookeeper
end-collector.sources.<%= "#{src}" %>.dualCommit = false
#Single Channel
end-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume/locker
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
end-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["flume_collector"]["kafka_brokers"]['dfw2'] %>
end-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-photon-<%= details[:'consumer_group'] %>
end-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
end-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
end-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
end-collector.sources.<%= "#{src}" %>.consumerGroupSize = <%= @mergesrc_consumer_gpsize %>
end-collector.sources.<%= "#{src}" %>.kafka.auto.offset.reset = latest
end-collector.sources.<%= "#{src}" %>.audit.enable = true
end-collector.sources.<%= "#{src}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sources.<%= "#{src}" %>.audit.port = 2530
end-collector.sources.<%= "#{src}" %>.tier = merger_src
end-collector.sources.<%= "#{src}" %>.interceptors = topic-name-prefix-adder-<%= "#{src}" %>
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.type = org.apache.flume.source.kafka.TopicNamePrefixAdder$Builder
end-collector.sources.<%= "#{src}" %>.interceptors.topic-name-prefix-adder-<%= "#{src}" %>.topicHeaderKey = category
#Kerberos Conf
end-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
end-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
end-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>
<% end -%>

<% @merged_kafka_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kafka.PooledKafkaSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.topicHeader = category
end-collector.sinks.<%= "#{sink}" %>.flumeBatchSize = 500
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.client.id = <%= details[:producer_id] %>
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.buffer.memory = 268435456
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.batch.size = 65536
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.linger.ms = 5
end-collector.sinks.<%= "#{sink}" %>.kafka.bootstrap.servers = <%= @kafkabrokers %>
end-collector.sinks.<%= "#{sink}" %>.kafka.topic = dummy
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.acks = all
# valid values none, gzip, snappy
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.compression.type = gzip
#Single channel
end-collector.sinks.<%= "#{sink}" %>.workerThreads = <%= @mergesinkworkerthreads %>
end-collector.sinks.<%= "#{sink}" %>.audit.enable = true
end-collector.sinks.<%= "#{sink}" %>.audit.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sinks.<%= "#{sink}" %>.audit.port = 2530
end-collector.sinks.<%= "#{sink}" %>.tier = merger_sink
#Kerberos Conf
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.security.protocol = SASL_PLAINTEXT
end-collector.sinks.<%= "#{sink}" %>.kafka.producer.sasl.kerberos.service.name = kafka
<% end %>

<% @platinum_hdfs_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/flume/databus/platinummerge
end-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/databus/flume/merge/temp
end-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/databus/flume/merge/staging
end-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/databus/streams
end-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
end-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 256000000
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
end-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
end-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
end-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
end-collector.sinks.<%= "#{sink}" %>.hdfs.callTimeout = 10000
end-collector.sinks.<%= "#{sink}" %>.hdfs.filePrefix = <%= node["hostname"] %>
end-collector.sinks.<%= "#{sink}" %>.proxyUser = databus
end-collector.sinks.<%= "#{sink}" %>.uploaderPoolSize = 10
end-collector.sinks.<%= "#{sink}" %>.defaultUploaderSleepIntervalMs = 1000
#Single Channel
end-collector.sinks.<%= "#{sink}" %>.backoffThreshold = 10.0
end-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /merge/platinumpromoter
end-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /merge/platinumretention
end-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /merge/platinumaudit
end-collector.sinks.<%= "#{sink}" %>.scribe.host = <%= @flumeagent %>
end-collector.sinks.<%= "#{sink}" %>.scribe.port = 2530
end-collector.sinks.<%= "#{sink}" %>.codeC = gzip
end-collector.sinks.<%= "#{sink}" %>.tier = global_mbs_sink
end-collector.sinks.<%= "#{sink}" %>.retention.topics = <%= @platinum_retention_topics %>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.tpce_enriched_download = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.tpce_custom_goal_summary = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.tpce_purchase_summary = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.perfRR = 96
#end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @platinumzookeeper %>
end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
#promotion enabling / disabling
end-collector.sinks.<%= "#{sink}" %>.promotion.enable = <%= details[:ispromoter] %>
<% if @colo == 'dfw1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'pek1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'ams1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'maa1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'dfw2' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosKeytab = <%= @keytab %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosPrincipal = flume/<%= node["fqdn"] %>@PROD.INMOBI.COM
<% end %>

<% @local_hdfs_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/flume/databus/local
end-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/databus/flume/local/temp
end-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/databus/flume/local/staging
end-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/databus/streams_local
end-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
end-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 256000000
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
end-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
end-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
end-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
end-collector.sinks.<%= "#{sink}" %>.hdfs.callTimeout = 10000
end-collector.sinks.<%= "#{sink}" %>.hdfs.filePrefix = <%= node["hostname"] %>
end-collector.sinks.<%= "#{sink}" %>.proxyUser = databus
end-collector.sinks.<%= "#{sink}" %>.uploaderPoolSize = 20
end-collector.sinks.<%= "#{sink}" %>.defaultUploaderSleepIntervalMs = 1000
#Single Channel
end-collector.sinks.<%= "#{sink}" %>.backoffThreshold = 30.0
end-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /local/promoter
end-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /local/retention
end-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /local/audit
end-collector.sinks.<%= "#{sink}" %>.scribe.host = <%= @flumeagent %>
end-collector.sinks.<%= "#{sink}" %>.scribe.port = 2530
end-collector.sinks.<%= "#{sink}" %>.codeC = gzip
end-collector.sinks.<%= "#{sink}" %>.tier = lbs_sink
end-collector.sinks.<%= "#{sink}" %>.retention.topics = <%= @local_retention_topics %>
end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosKeytab = <%= @keytab %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosPrincipal = flume/<%= node["fqdn"] %>@PROD.INMOBI.COM
<% end %>
<% @merged_hdfs_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/flume/databus/merge
end-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/databus/flume/merge/temp
end-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/databus/flume/merge/staging
end-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/databus/streams
end-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
end-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 256000000
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
end-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
end-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
end-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
end-collector.sinks.<%= "#{sink}" %>.hdfs.callTimeout = 10000
end-collector.sinks.<%= "#{sink}" %>.hdfs.filePrefix = <%= node["hostname"] %>
end-collector.sinks.<%= "#{sink}" %>.proxyUser = databus
end-collector.sinks.<%= "#{sink}" %>.uploaderPoolSize = 10
end-collector.sinks.<%= "#{sink}" %>.defaultUploaderSleepIntervalMs = 1000
#Single Channel
end-collector.sinks.<%= "#{sink}" %>.backoffThreshold = 10.0
end-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /merge/promoter
end-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /merge/retention
end-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /merge/audit
end-collector.sinks.<%= "#{sink}" %>.scribe.host = <%= @flumeagent %>
end-collector.sinks.<%= "#{sink}" %>.scribe.port = 2530
end-collector.sinks.<%= "#{sink}" %>.codeC = gzip
end-collector.sinks.<%= "#{sink}" %>.tier = mbs_sink
end-collector.sinks.<%= "#{sink}" %>.retention.topics = <%= @merge_retention_topics %>
<% if @colo == 'dfw1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_dfw1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_dfw1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_dfw1 = 120
<% end %>
<% if @colo == 'lhr1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_lhr1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_lhr1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_lhr1 = 120
<% end %>
<% if @colo == 'pek1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_pek1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_pek1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_pek1 = 120
<% end %>
end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosKeytab = <%= @keytab %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosPrincipal = flume/<%= node["fqdn"] %>@PROD.INMOBI.COM
<% end %>


<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks1.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.eventhub.EventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = iad2-adroit
end-collector.sinks.<%= "#{sink}" %>.eventHubName = sg-ingestion-iad2
end-collector.sinks.<%= "#{sink}" %>.batchsize = 204800
end-collector.sinks.<%= "#{sink}" %>.saskeyname = sg-ingestion-iad2-write
end-collector.sinks.<%= "#{sink}" %>.saskey = czGSP4jz7FfKpJlSL8FDdCFPRG2NyM9VQn/SOVtLYH4=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.workercount = 10
end-collector.sinks.<%= "#{sink}" %>.transactionsize = 450
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks2.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.eventhub.EventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = iad2-adroit
end-collector.sinks.<%= "#{sink}" %>.eventHubName = organic-purchase-iad2
end-collector.sinks.<%= "#{sink}" %>.batchsize = 204800
end-collector.sinks.<%= "#{sink}" %>.saskeyname = organic-purchase-iad2-write
end-collector.sinks.<%= "#{sink}" %>.saskey = rDRmorAFNA5/UY9Ki3q7yL3xTU9XfizTQijkH2ie3m8=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.workercount = 10
end-collector.sinks.<%= "#{sink}" %>.transactionsize = 450
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks3.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.eventhub.EventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = iad2-adroit
end-collector.sinks.<%= "#{sink}" %>.eventHubName = organic-install-iad2
end-collector.sinks.<%= "#{sink}" %>.batchsize = 204800
end-collector.sinks.<%= "#{sink}" %>.saskeyname = organic-install-iad2-write
end-collector.sinks.<%= "#{sink}" %>.saskey = T/6BWFziloCFP1TO5xG+PQHHtDbtC0KV9or8+2OfYmA=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.workercount = 10
end-collector.sinks.<%= "#{sink}" %>.transactionsize = 450
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks4.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.eventhub.EventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = iad2-adroit
end-collector.sinks.<%= "#{sink}" %>.eventHubName = organic-custom-iad2
end-collector.sinks.<%= "#{sink}" %>.batchsize = 204800
end-collector.sinks.<%= "#{sink}" %>.saskeyname = organic-custom-iad2-write
end-collector.sinks.<%= "#{sink}" %>.saskey = exrbLkUlv7yZ8RNcY8SxC1Iq+tmUi2LPaXVIJv70q24=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.workercount = 10
end-collector.sinks.<%= "#{sink}" %>.transactionsize = 450
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks5.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.eventhub.EventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = iad2-adroit
end-collector.sinks.<%= "#{sink}" %>.eventHubName = inorganic-install-iad2
end-collector.sinks.<%= "#{sink}" %>.batchsize = 204800
end-collector.sinks.<%= "#{sink}" %>.saskeyname = inorganic-install-iad2-write
end-collector.sinks.<%= "#{sink}" %>.saskey = JEgB9bjOXMXale6NzzqfBpSWoNKy768SHnmq15ktkSQ=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.workercount = 10
end-collector.sinks.<%= "#{sink}" %>.transactionsize = 450
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks6.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = organic-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = VAy6d8GYBsXCJBIZFGIUWvL9ESO/sFYSOPCrKN39n8Y=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = organic-custom-backfill
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks7.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = organic-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = HajEk3ZqqGz7ZG9pMxVGKHl2d3PdxKhDiLSIbmC7GxA=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = organic-install-backfill
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks8.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = organic-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = icIKeLrnwzpM07+5m2YcMdXeUnZPdfyoWiOZw0AA+3k=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = organic-purchase-backfill
<% end %>
<% end %>

<% if @colo == 'dfw1' -%>
<% @dfw1_eventhub_sinks9.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = iapstreams
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-iapclick
end-collector.sinks.<%= "#{sink}" %>.saskey = b+7IoE/2q3Dznp1r841kceyatzHrZohyAymg+3MlAOY=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = iapclick
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks1.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = organic-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = obqAyh5WordpXfQ0jFoGHzxy8Msi3rgJc6Cmmjppy+M=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = organic-install
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks2.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = postserve-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = bZMU2agKtDZ5t+puQkro3+NqLWWa5sluHg5ifHKCuOU=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = inorganic-install
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks3.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = organic-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = nveQUMtio8y5zI9VCDMQhSfa7F2cRJkHTCYOnBcsAKc=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = organic-purchase
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks4.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = postserve-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = wrsL8eDzwjQxEdwDiyPN/7JmOks7uNIsi4sRl40t61w=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = inorganic-purchase
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks5.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = organic-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = rL8+m1Gmx0c+4WinqLVfSw/U+wzX4tvYExT/vJB3+gE=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = organic-custom
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks6.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = postserve-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = DuWmHVgyfOnB78RPFxd39U+wjpWtAh21FHvNisn9zPU=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = inorganic-custom
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks7.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = cdp-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy-useast
end-collector.sinks.<%= "#{sink}" %>.saskey = nL41ShL+Xrt/ohoGWsNFMDr1rSVi+8F/62MgXycnlEU=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = user-segment
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_sinks8.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.kestrel.KestrelEventHubSink
end-collector.sinks.<%= "#{sink}" %>.namespace = cdp-ehub-useast
end-collector.sinks.<%= "#{sink}" %>.saskeyname = write-policy
end-collector.sinks.<%= "#{sink}" %>.saskey = kGBtBtXuxUyri88e3+YQNrZNucmArmBAjCMfFbFlpdY=
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.publisherworkercount = 10
end-collector.sinks.<%= "#{sink}" %>.compression.enable = false
end-collector.sinks.<%= "#{sink}" %>.lingertime = 50
end-collector.sinks.<%= "#{sink}" %>.topic.name = optout-users
<% end %>
<% end %>

<% @merge_avroreceive_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 500
<% end %>
<% @local_hdfs_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 10000
<% end %>
<% @merge_hdfs_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 10000
<% end %>
<% @platinum_hdfs_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 10000
<% end %>

<% if @colo == 'dfw1' -%>
<% @local_eventhub_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
#<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.capacity = 50000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 500
end-collector.channels.<%= "#{channel}" %>.spoolWorkers = 8
end-collector.channels.<%= "#{channel}" %>.despoolWorkers = 8
<% end %>
<% end %>

<% if @colo == 'dfw2' -%>
<% @local_eventhub_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 50000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 500
end-collector.channels.<%= "#{channel}" %>.spoolWorkers = 8
end-collector.channels.<%= "#{channel}" %>.despoolWorkers = 8
<% end %>
<% end %>

#################################################################################
# flume secure configs
<% @platinum_secure_hdfs_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/secure/flume/databus/platinummerge
end-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/flume/merge/temp
end-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/flume/merge/staging
end-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/streams
end-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
end-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 256000000
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
end-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
end-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
end-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
end-collector.sinks.<%= "#{sink}" %>.hdfs.callTimeout = 10000
end-collector.sinks.<%= "#{sink}" %>.hdfs.filePrefix = <%= node["hostname"] %>
end-collector.sinks.<%= "#{sink}" %>.proxyUser = databus
end-collector.sinks.<%= "#{sink}" %>.uploaderPoolSize = 10
end-collector.sinks.<%= "#{sink}" %>.defaultUploaderSleepIntervalMs = 1000
#Single Channel
end-collector.sinks.<%= "#{sink}" %>.backoffThreshold = 10.0
end-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /secure/merge/platinumpromoter
end-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /secure/merge/platinumretention
end-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /secure/merge/platinumaudit
end-collector.sinks.<%= "#{sink}" %>.scribe.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sinks.<%= "#{sink}" %>.scribe.port = 2530
end-collector.sinks.<%= "#{sink}" %>.codeC = gzip
end-collector.sinks.<%= "#{sink}" %>.tier = global_mbs_sink
end-collector.sinks.<%= "#{sink}" %>.retention.topics = <%= @platinum_retention_topics %>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.tpce_enriched_download = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.tpce_custom_goal_summary = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.tpce_purchase_summary = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_enriched_pb_dfw1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_enriched_pb_dfw2 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_enriched_pb_pek1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_enriched_pb_ams1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_enriched_pb_maa1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_nonenriched_pb_dfw1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_nonenriched_pb_dfw2 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_nonenriched_pb_ams1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_nonenriched_pb_maa1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_nonenriched_pb_pek1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.ifc_photon_enriched_san_pb = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.perfex_beacon_ev1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.perfex_click_ev1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.perfex_render_ev1 = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.normalized_postback = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_download_event = 168
end-collector.sinks.<%= "#{sink}" %>.retention.topics.perfRR = 96
#end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @platinumzookeeper %>
end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
#promotion enabling / disabling
end-collector.sinks.<%= "#{sink}" %>.promotion.enable = <%= details[:ispromoter] %>
<% if @colo == 'dfw1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'pek1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'ams1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'maa1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
<% if @colo == 'dfw2' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.beeswax_bid_logs = 240
end-collector.sinks.<%= "#{sink}" %>.retention.topics.rtf = 4560
<% end %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosKeytab = <%= @keytab %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosPrincipal = flume/<%= node["fqdn"] %>@PROD.INMOBI.COM
<% end %>

<% @local_secure_hdfs_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/secure/flume/databus/local
end-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/flume/local/temp
end-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/flume/local/staging
end-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/streams_local
end-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
end-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 256000000
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
end-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
end-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
end-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
end-collector.sinks.<%= "#{sink}" %>.hdfs.callTimeout = 10000
end-collector.sinks.<%= "#{sink}" %>.hdfs.filePrefix = <%= node["hostname"] %>
end-collector.sinks.<%= "#{sink}" %>.proxyUser = databus
end-collector.sinks.<%= "#{sink}" %>.uploaderPoolSize = 20
end-collector.sinks.<%= "#{sink}" %>.defaultUploaderSleepIntervalMs = 1000
#Single Channel
end-collector.sinks.<%= "#{sink}" %>.backoffThreshold = 30.0
end-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /secure/local/promoter
end-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /secure/local/retention
end-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /secure/local/audit
end-collector.sinks.<%= "#{sink}" %>.scribe.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sinks.<%= "#{sink}" %>.scribe.port = 2530
end-collector.sinks.<%= "#{sink}" %>.codeC = gzip
end-collector.sinks.<%= "#{sink}" %>.tier = lbs_sink
end-collector.sinks.<%= "#{sink}" %>.retention.topics = <%= @local_retention_topics %>
end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosKeytab = <%= @keytab %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosPrincipal = flume/<%= node["fqdn"] %>@PROD.INMOBI.COM

<% end %>
<% @merged_secure_hdfs_sinks.each do |sink, details| %>
end-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
end-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/secure/flume/databus/merge
end-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/flume/merge/temp
end-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/flume/merge/staging
end-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/secure/projects/databus/streams
end-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
end-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 256000000
end-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
end-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
end-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
end-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
end-collector.sinks.<%= "#{sink}" %>.hdfs.callTimeout = 10000
end-collector.sinks.<%= "#{sink}" %>.hdfs.filePrefix = <%= node["hostname"] %>
end-collector.sinks.<%= "#{sink}" %>.proxyUser = databus
end-collector.sinks.<%= "#{sink}" %>.uploaderPoolSize = 10
end-collector.sinks.<%= "#{sink}" %>.defaultUploaderSleepIntervalMs = 1000
#Single Channel
end-collector.sinks.<%= "#{sink}" %>.backoffThreshold = 10.0
end-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /secure/merge/promoter
end-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /secure/merge/retention
end-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /secure/merge/audit
end-collector.sinks.<%= "#{sink}" %>.scribe.host = flume-audit.grid.dfw1.inmobi.com
end-collector.sinks.<%= "#{sink}" %>.scribe.port = 2530
end-collector.sinks.<%= "#{sink}" %>.codeC = gzip
end-collector.sinks.<%= "#{sink}" %>.tier = mbs_sink
end-collector.sinks.<%= "#{sink}" %>.retention.topics = <%= @merge_retention_topics %>
<% if @colo == 'dfw1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_dfw1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_dfw1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_dfw1 = 120
<% end %>
<% if @colo == 'pek1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_pek1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_pek1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_pek1 = 120
<% end %>
<% if @colo == 'ams1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_ams1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_ams1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_ams1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_lhr1 = 120
<% end %>
<% if @colo == 'maa1' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_maa1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_maa1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_maa1 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_lhr1 = 120
<% end %>
<% if @colo == 'dfw2' -%>
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpc_dfw2 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_cpm_dfw2 = 120
end-collector.sinks.<%= "#{sink}" %>.retention.topics.billing_download_dfw2 = 120
<% end %>
end-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosKeytab = <%= @keytab %>
end-collector.sinks.<%= "#{sink}" %>.hdfs.kerberosPrincipal = flume/<%= node["fqdn"] %>@PROD.INMOBI.COM
<% end %>


<% @local_secure_hdfs_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 10000
<% end %>
<% @merge_secure_hdfs_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 10000
<% end %>
<% @platinum_secure_hdfs_channels.each do |channel| %>
end-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
#Single Channel
end-collector.channels.<%= "#{channel}" %>.capacity = 100000
end-collector.channels.<%= "#{channel}" %>.spoolOnlyOnShutdown = true
end-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
end-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
end-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 10000
<% end %>

