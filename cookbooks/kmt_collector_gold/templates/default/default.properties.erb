app.name=kmt
mirroring.namespace=GoldCollector
spooling.path=/data/inmobi/kafkamirroring/spooling

#HDFSProducer Flags
hdfs.principal.name=kmt
hdfs.expiry.mins=5

#hdfs.error.location=kafka/error
#hdfs.temp.location=kafka/working
#hdfs.expiry.location=kafka/expiry

#Limits
hdfs.folder.limit=1000
hdfs.batch.flush=10000
hdfs.file.limit=256000000

#In case of lot files piled and wants to clean them, using thread pool.
local.shifterpool.size=5
#If more than 3 mins files found, they will be marked as invalid or old depending up the state of the file.
local.staging.threshold=1
local.staging.dealy=5
local.staging.path=/data/kafka/staging/

#timer tasks
populate.streams=300
cleanup.deadnodes=120
try.streams=120
clean.streams=120
reaload.config=120
streams.perrun=20

#zk/file/hdfs
zk.basesleeptime=1000
zk.maxretries=3
offset.read=zk
offset.file.location=/data/inmobi/kafkamirroring/offsets

channelpool.size=10
enable.zero.bytes.file=false
auto.commit.interval=10000
reload.newpartitions=300

enable.audit=false

hdfs.retries=10
hdfs.write.retries=1
hdfs.filecopy.retries=100
hdfs.retry.wait.interval=10000
#If a job is inprogress for more than 10mins, this should be alerted
inprogress.jobs.oldage=10
#If a file is more than 60 mins, this should be alerted
oldfiles.age=60
#Maximum number jobs allowed per node
max.jobspernode=200

prod.colos=<%= @collector_id %>
<%= @collector_id %>.config={"configs":[{"name":"<%= @colo %>","url":"hdfs://<%= @hdfs_id %>:8020","regex":"hdfs://[a-z]*:[0-9]*"}<% if @colo  == 'uh1' -%>,{"name":"uh1","url":"hdfs://gold:8020","regex":"hdfs://[a-z]*:[0-9]*"}<% end -%><% if @colo  == 'dfw1' -%>,{"name":"dfw1","url":"hdfs://platinum:8020","regex":"hdfs://[a-z]*:[0-9]*"}<% end -%>]}
