
app.name=kmt
mirroring.namespace=Mirroring
spooling.path=/data/inmobi/kafkamirroring/spooling

#HDFSProducer Flags
hdfs.principal.name=kmt
hdfs.expiry.mins=5

#Testing purpose (dont edit these properties)
#throwexception.partitionid=0
#throwexception.afterevents=-1

#Limits
kmt.batch.size=5000
hdfs.retries=10
hdfs.write.retries=1
hdfs.filecopy.retries=100
hdfs.retry.wait.interval=10000

hdfs.folder.limit=1000
hdfs.file.limit=128000000

#If a job is inprogress for more than 10mins, this should be alerted
inprogress.jobs.oldage=10
#If a file is more than 60 mins, this should be alerted
oldfiles.age=60
#Maximum number jobs allowed per node
max.jobspernode=100

#In case of lot files piled and wants to clean them, using thread pool.
local.shifterpool.size=5
#If more than 5 mins files found, they will be marked as invalid or old depending up the state of the file.
local.staging.delay=5
local.staging.threshold=1
local.staging.path=/data/kafka/staging/

#timer tasks
#600 sec for populating pending Q
populate.streams=300
#120 sec for picking new jobs from pending Q
try.streams=120
#If a node is in zk entry, but actually not running
cleanup.deadnodes=120
clean.streams=120
#Report every 120 sec node level info to graphite
reaload.config=120
streams.perrun=10

#zk/file/hdfs
zk.maxretries=3
zk.basesleeptime=1000
offset.read=zk
offset.file.location=/data/inmobi/kafkamirroring/offsets

enable.audit=false
channelpool.size=10
enable.zero.bytes.file=false
auto.commit.interval=10000
reload.newpartitions=300

dev.colos=corp,uh1,lhr1,hkg1
corp.config={"configs":[\
  {"name":"corp","url":"hdfs://localhost:9000","regex":"hdfs://[a-z]*:[0-9]*"},\
  {"name":"corp", "url":"hdfs://localhost:9009","regex":"hdfs://[a-z]*:[0-9]*"}]}

prod.colos=lhr1
lhr1.config={"configs":[{"name":"lhr1","url":"hdfs://azure:8020","regex":"hdfs://[a-z]*:[0-9]*"}]}
