# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.


# The configuration file needs to define the sources, 
# the channels and the sinks.
# Sources, channels and sinks are defined per agent, 
# in this case called 'audit-collector'

####################################################################################################
#avro source => disk backed memory => kafka
####################################################################################################

audit-collector.sources =<% @sources.each do |src, details| %><%= " #{src}" %><% end %>
audit-collector.channels = <%= @allchannels %>
audit-collector.sinks = <%= @allsinks %>


<% @sources.each do |src, details| %>
<% if details[:'src_category'] == 'auditkafka' -%>
audit-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
audit-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
audit-collector.sources.<%= "#{src}" %>.selector.type = <%= details[:'selector.type'] %>
audit-collector.sources.<%= "#{src}" %>.selector.optional = <%= details[:selector_optional] %>
audit-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= @kafkazookeeper %>
audit-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= @kafkabrokers %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-<%= details[:'consumer_group'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
audit-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:'batchSize'] %>
audit-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.auto.offset.reset = latest
audit-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume-audit/locker
<% end -%>
<% if details[:'src_category'] == 'auditscribe' -%>
audit-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
audit-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
audit-collector.sources.<%= "#{src}" %>.selector.optional = <%= details[:selector_optional] %>
audit-collector.sources.<%= "#{src}" %>.port = <%= details[:port] %>
audit-collector.sources.<%= "#{src}" %>.maxReadBufferBytes = 67108864
audit-collector.sources.<%= "#{src}" %>.workerThreads = 100
<% end -%>
<% if details[:'src_category'] == 'pek1kafkamerge' -%>
audit-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
audit-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
audit-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["audit_collector"]["kafka_zookeeper"]['pek1'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
audit-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["audit_collector"]["kafka_brokers"]['pek1'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-<%= details[:'consumer_group'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
audit-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
audit-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.auto.offset.reset = latest
audit-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume-audit/locker
#Kerberos Conf
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>
<% if details[:'src_category'] == 'dfw2kafkamerge' -%>
audit-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
audit-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
audit-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["audit_collector"]["kafka_zookeeper"]['dfw2'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
audit-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["audit_collector"]["kafka_brokers"]['dfw2'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-<%= details[:'consumer_group'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
audit-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
audit-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.auto.offset.reset = latest
audit-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume-audit/locker
#Kerberos Conf
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>
<% if details[:'src_category'] == 'ams1kafkamerge' -%>
audit-collector.sources.<%= "#{src}" %>.type = <%= details[:type] %>
audit-collector.sources.<%= "#{src}" %>.channels = <%= details[:channels] %>
audit-collector.sources.<%= "#{src}" %>.zookeeper.connect = <%= node["audit_collector"]["kafka_zookeeper"]['ams1'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.connection.timeout.ms=30000
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.zookeeper.session.timeout.ms=30000
audit-collector.sources.<%= "#{src}" %>.kafka.bootstrap.servers = <%= node["audit_collector"]["kafka_brokers"]['ams1'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.group.id = flume-collector-<%= details[:'consumer_group'] %>
audit-collector.sources.<%= "#{src}" %>.kafka.topics = <%= details[:'kafka.topics'] %>
audit-collector.sources.<%= "#{src}" %>.batchSize = <%= details[:batchSize] %>
audit-collector.sources.<%= "#{src}" %>.topicHeaderKey = category
audit-collector.sources.<%= "#{src}" %>.kafka.consumer.auto.offset.reset = latest
audit-collector.sources.<%= "#{src}" %>.sourceLockDir = /data/d1/flume-audit/locker
#Kerberos Conf
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.security.protocol = SASL_PLAINTEXT
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.sasl.kerberos.service.name = kafka
#audit-collector.sources.<%= "#{src}" %>.kafka.consumer.session.timeout.ms = 30000
<% end -%>
<% end -%>

<% @hdfs_sinks.each do |sink, details| %>
audit-collector.sinks.<%= "#{sink}" %>.type = org.apache.flume.sink.hdfs.HDFSEventSink
audit-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
audit-collector.sinks.<%= "#{sink}" %>.hdfs.path = file:///data/d1/flume-audit/local
audit-collector.sinks.<%= "#{sink}" %>.hdfs.temp.path = hdfs://<%= details[:cluster] %>/data/audit/working/temp
audit-collector.sinks.<%= "#{sink}" %>.hdfs.staging.path = hdfs://<%= details[:cluster] %>/data/audit/working/staging
audit-collector.sinks.<%= "#{sink}" %>.hdfs.final.path = hdfs://<%= details[:cluster] %>/data/audit/streams
audit-collector.sinks.<%= "#{sink}" %>.hdfs.threadsPoolSize = 1
audit-collector.sinks.<%= "#{sink}" %>.hdfs.batchSize = 500
audit-collector.sinks.<%= "#{sink}" %>.hdfs.rollInterval = 60
audit-collector.sinks.<%= "#{sink}" %>.hdfs.rollSize = 512000000
audit-collector.sinks.<%= "#{sink}" %>.hdfs.rollCount = 0
audit-collector.sinks.<%= "#{sink}" %>.hdfs.inUseSuffix = .processing
audit-collector.sinks.<%= "#{sink}" %>.audit.enable = false
audit-collector.sinks.<%= "#{sink}" %>.hdfs.fileType = DataStream
audit-collector.sinks.<%= "#{sink}" %>.serializer = BASE64
audit-collector.sinks.<%= "#{sink}" %>.promoter.service.zkPath = /data/audit/promoter
audit-collector.sinks.<%= "#{sink}" %>.retention.service.zkPath = /data/audit/retention
audit-collector.sinks.<%= "#{sink}" %>.audit.service.zkPath = /data/audit/audit
audit-collector.sinks.<%= "#{sink}" %>.retention.topics = _audit
audit-collector.sinks.<%= "#{sink}" %>.zk.url = <%= @kafkazookeeper %>
audit-collector.sinks.<%= "#{sink}" %>.codeC = gzip
audit-collector.sinks.<%= "#{sink}" %>.tier = LOCAL
audit-collector.sinks.<%= "#{sink}" %>.retention.topics._audit = 120
<% end %>
<% @druid_sinks.each do |sink, details| %>
audit-collector.sinks.<%= "#{sink}" %>.type = com.apache.flume.sink.druid.DruidSink
audit-collector.sinks.<%= "#{sink}" %>.channel = <%= details[:channel] %>
audit-collector.sinks.<%= "#{sink}" %>.indexService = overlord
audit-collector.sinks.<%= "#{sink}" %>.discoveryPath = druid/discovery
audit-collector.sinks.<%= "#{sink}" %>.dimensions = timeinterval,hostname,tier,topic,cluster
audit-collector.sinks.<%= "#{sink}" %>.timestampField = timeinterval
audit-collector.sinks.<%= "#{sink}" %>.queryGranularity = minute
audit-collector.sinks.<%= "#{sink}" %>.dataSource = conduitaudit
audit-collector.sinks.<%= "#{sink}" %>.batchSize = 500
audit-collector.sinks.<%= "#{sink}" %>.aggregators = sent,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c15,c30,c60,c120,c240,c600,totalevents
audit-collector.sinks.<%= "#{sink}" %>.zookeeperLocation = <%= details[:zookeeper] %>
audit-collector.sinks.<%= "#{sink}" %>.period = PT22H
audit-collector.sinks.<%= "#{sink}" %>.segmentGranularity = DAY
audit-collector.sinks.<%= "#{sink}" %>.blacklistedTiers = pintailpublisher,pintailconsumer
<% end %>

<% @druid_channels.each do |channel| %>
audit-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
audit-collector.channels.<%= "#{channel}" %>.capacity = 200000
audit-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
audit-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
audit-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 1000000
audit-collector.channels.<%= "#{channel}" %>.spoolWorkers = 8
audit-collector.channels.<%= "#{channel}" %>.despoolWorkers = 8
<% end %>
<% @hdfs_channels.each do |channel| %>
audit-collector.channels.<%= "#{channel}" %>.type = org.apache.flume.channel.DiskBackedMemoryChannel
audit-collector.channels.<%= "#{channel}" %>.capacity = 200000
audit-collector.channels.<%= "#{channel}" %>.transactionCapacity = 500
audit-collector.channels.<%= "#{channel}" %>.spoolDirectories = <%= @spooldir %>/<%= "#{channel}" %>
audit-collector.channels.<%= "#{channel}" %>.spoolDiskCapacityMB = 1000000
audit-collector.channels.<%= "#{channel}" %>.spoolWorkers = 8
audit-collector.channels.<%= "#{channel}" %>.despoolWorkers = 8
<% end %>
